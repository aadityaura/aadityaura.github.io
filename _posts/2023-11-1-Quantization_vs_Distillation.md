---
layout: paper-note
title: "Quantized vs. Distilled Neural Models: A Comparison"
description: A dive into the techniques of quantizing and distilling deep learning models: What are they and how do they differ?
date: 2023-11-1

bibliography: paper-notes.bib

---



Deep learning models, especially those with vast parameters, pose challenges for deployment in resource-constrained environments. Two popular techniques, quantization and distillation, address this issue, aiming to make these models more lightweight without compromising too much on performance. But what do they entail, and how do they compare?
